{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "xq3guyo2imrnu2i6rt6n",
   "authorId": "1743993337498",
   "authorName": "MSMAISA",
   "authorEmail": "maria.zvezdkina@gmail.com",
   "sessionId": "7dcfe547-ca62-4ac1-9142-891997fa9b7c",
   "lastEditTime": 1762111216422
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e3fdc8-8480-41c1-8d8a-6f7eb7101ad7",
   "metadata": {
    "collapsed": false,
    "name": "Setup"
   },
   "source": [
    "# Initialize Environment for Distributed Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0dd10-192c-4f79-940c-ca675028b32c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": true,
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "# Some of the features in this HOL are in preview. Pin the version of snowflake-ml-python for reproducibility. \n",
    "# NOTE - you do not need to restart the kernel since we're running this before importing snowflake-ml-python.\n",
    "!pip install snowflake-ml-python==1.9.0\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c17443-8a67-4250-9d12-6e08ab614811",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "!pip freeze | grep keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3300af7-7d13-43e6-b05b-0b20d3ffa5ce",
   "metadata": {
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": [
    "!pip freeze | grep snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91117e3b-e018-45ba-a547-0001558b0c5a",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "context = ray.data.DataContext.get_current()\n",
    "context.execution_options.verbose_progress = False\n",
    "context.enable_operator_progress_bars = False\n",
    "context.enable_progress_bars = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cf1ec-6b27-4604-9f81-119455b1d4d5",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.runtime_cluster import scale_cluster\n",
    "\n",
    "# Scale out the notebook to have multiple nodes available for execution\n",
    "SCALE_FACTOR = 2\n",
    "scale_cluster(SCALE_FACTOR)\n",
    "\n",
    "# Sync the python env to the scaled out cluster.\n",
    "from runtime_env import python_env\n",
    "python_env.sync_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0ea63-361a-49b8-a001-f70ecfa800cd",
   "metadata": {
    "collapsed": false,
    "name": "Process_Reviews"
   },
   "source": [
    "# Process Review Text Data\n",
    "- Load reviews with `SFStageTextDataSource`\n",
    "- Parse review text with Ray data"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b33ed52-3dc0-42d6-8cb2-2a6ac80f78fd",
   "metadata": {
    "language": "python",
    "name": "SYNTHDATA"
   },
   "outputs": [],
   "source": "# Load the synthetic data using Snowpark\nsynth_data = session.table(\"REVIEWS.PUBLIC.SYNTHDATA\")\n\n# Convert to Pandas DataFrame\nsynth_df = synth_data.to_pandas()\n\n# Convert to Ray Dataset\nray_dataset = ray.data.from_pandas(synth_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# ❌ DEPRECATED INGESTION STEP — DO NOT RUN\n# This block was used to ingest raw .txt files from a Snowflake stage named \"REVIEWS\".\n# It has been replaced by direct table access using REVIEWS.PUBLIC.SYNTHDATA.\n# Retained here for reference only — skip execution.\n\n# use ray data to process sentiment \nfrom snowflake.ml.ray.datasource import SFStageTextDataSource\n\nfile_name = \"*.txt\"\nstage_name = \"REVIEWS\"\n\ntext_source = SFStageTextDataSource(\n    stage_location=stage_name,\n    file_pattern=file_name\n)\n\ntext_dataset = ray.data.read_datasource(text_source)\n"
  },
  {
   "cell_type": "code",
   "id": "edebf038-f14e-4ace-9c78-33a0e1a452a0",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "# ✅ INGEST REVIEW TEXT FROM SPECIFIC FILE IN STAGE\n# This block loads the file 'synthetic_review_data_text.txt' from the stage @\"HOL_DB\".\"HOL_SCHEMA\".\"REVIEWS\"\n# Each line should follow the format: \"UUID\",\"REVIEW_TEXT\"\n\nfrom snowflake.ml.ray.datasource import SFStageTextDataSource\n\n# Session must be scoped to HOL_DB and HOL_SCHEMA\nsession.use_database(\"HOL_DB\")\nsession.use_schema(\"HOL_SCHEMA\")\n\n# Use session-scoped stage and wildcard pattern\nstage_location = \"@REVIEWS\"\nfile_pattern = \"*.txt\"\n\ntext_source = SFStageTextDataSource(\n    stage_location=stage_location,\n    file_pattern=file_pattern\n)\n\ntext_dataset = ray.data.read_datasource(text_source)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca40568b-1e1a-47be-9222-824523987301",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom snowflake.ml.ray.datasource import SFStageTextDataSource\n\n# Step 1: Ingest raw .txt reviews from stage\ntext_source = SFStageTextDataSource(\n    stage_location=\"REVIEWS\",\n    file_pattern=\"*.txt\"\n)\ntext_dataset = ray.data.read_datasource(text_source)\n\n# Step 2: Parse each line into UUID and REVIEW_TEXT\ndef parse_reviews(batch):\n    parsed_data = {}\n    value = batch[\"text\"]\n    parts = value.split('\",\"', 1)\n    parsed_data['UUID'] = parts[0].strip('\"')\n    parsed_data['REVIEW_TEXT'] = parts[1].rstrip('\"')\n    return parsed_data\n\nparsed_dataset = text_dataset.map(parse_reviews)\n\n# Step 3: Convert parsed .txt reviews to Pandas and tag source\nparsed_df = parsed_dataset.to_pandas()\nparsed_df[\"SOURCE\"] = \"TXT\"\n\n# Step 4: Load synthetic structured reviews and tag source\nsynth_df = session.table(\"REVIEWS.PUBLIC.SYNTHDATA\").to_pandas()\nsynth_df[\"SOURCE\"] = \"SYNTH\"\n\n# Step 5: Combine both into one Ray dataset\ncombined_df = pd.concat([parsed_df, synth_df], ignore_index=True)\nray_dataset = ray.data.from_pandas(combined_df)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfae7c2-bc34-4a8d-a86f-e4a145e647e6",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ✅ ACTIVE PARSING STEP — required for .txt ingestion\n# This function extracts UUID and REVIEW_TEXT from raw lines in the .txt file.\n# Do not remove unless .txt ingestion is fully deprecated or replaced with structured formats.\n\ndef parse_reviews(batch):\n    \"\"\"\n    Parse reviews to extract UUID and review text from the input string.\n    \n    Args:\n        batch: Dictionary containing 'text' and 'file_name' keys\n        \n    Returns:\n        Dictionary with parsed UUID and review text\n    \"\"\"\n    # Initialize empty dictionary for results\n    parsed_data = {}\n    \n    value = batch[\"text\"]\n    # Split on the first occurrence of comma\n    parts = value.split('\",\"', 1)\n    \n    # Clean up the UUID (remove leading/trailing quotes)\n    uuid = parts[0].strip('\"')\n    \n    # Clean up the review text (remove trailing quote)\n    review_text = parts[1].rstrip('\"')\n    \n    # Store parsed values\n    parsed_data['UUID'] = uuid\n    parsed_data['REVIEW_TEXT'] = review_text\n        \n    return parsed_data\n\n# Apply the parsing function to the dataset\nparsed_dataset = text_dataset.map(parse_reviews)"
  },
  {
   "cell_type": "markdown",
   "id": "41fae9e1-749d-4675-bb64-2ead05c798ab",
   "metadata": {
    "collapsed": false,
    "name": "Review_Quality"
   },
   "source": [
    "# Predict Review Quality\n",
    "- Predict the quality with one-shot classification via HF pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9fdb90-1bae-4216-b910-b413f78e1b09",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "from transformers import pipeline\nimport numpy as np\n\n\nclass ModelPredictor:\n    def __init__(self):\n        # Load model\n        self.classifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\n\n    # define your batch operations    \n    def __call__(self, batch):\n        candidate_labels = ['detailed with specific information and experience', 'basic accurate information', 'generic brief with no details']\n        resp = self.classifier(batch[\"REVIEW_TEXT\"].tolist(), candidate_labels)\n\n        # Handle both resp and batch results\n        if isinstance(resp, dict):\n            raise ValueError(f\"Expected batch response, got {resp} for batch {batch['REVIEW_TEXT']}\")\n            \n        # Add results to batch\n        batch[\"REVIEW_QUALITY\"] = np.array([result[\"labels\"][np.argmax(result[\"scores\"])] for result in resp])\n        \n\n        return batch\n\n# ⛔ DEPRECATED: replaced by ray_dataset version below:\n# Apply batch operations to your dataset. HF Pipeline is itself a batch operation, \n#so we use Ray data just to scale across nodes, setting concurrency to number of nodes we have started.\n#dataset = parsed_dataset.map_batches(ModelPredictor, concurrency=4)\n\n# ✅ Replaced with \n# Apply model to combined Ray dataset\ndataset = ray_dataset.map_batches(ModelPredictor, concurrency=4)\n"
  },
  {
   "cell_type": "markdown",
   "id": "fa4b3b9c-3430-45db-92dc-bb1b29eae790",
   "metadata": {
    "collapsed": false,
    "name": "Store_Reviews"
   },
   "source": "# Store Processed Data in Snowflake"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424713b-83ff-43b9-9f62-d3ba3ce82bdf",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "# ⛔ DEPRECATED: replaced by native write_snowflake method\n# from snowflake.ml.ray.datasink.table_data_sink import SnowflakeTableDatasink\n# datasink = SnowflakeTableDatasink(\n#     table_name=\"SCORED_REVIEWS\",\n#     database_name=\"REVIEWS\",\n#     schema_name=\"PUBLIC\",\n#     auto_create_table=True,\n#     override=True\n# )\n\n\n# ⛔ BLOCKED: Ray actor failed due to network restrictions in free-tier environment\n# df = dataset.to_pandas()\n# session.write_pandas(df, table_name=\"SCORED_REVIEWS\", ...)\n# Convert Ray dataset to Pandas\n#df = dataset.to_pandas()\n\n# Write to Snowflake using Snowpark\n#session.write_pandas(\n#    df,\n#    table_name=\"SCORED_REVIEWS\",\n#    database=\"REVIEWS\",\n#    schema=\"PUBLIC\",\n#    overwrite=True\n#)\n\n\n# ⛔ ATTEMPT 3 FAILED: Hugging Face model could not be loaded due to lack of internet access\n# classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n# parsed_df[\"REVIEW_QUALITY\"] = parsed_df[\"REVIEW_TEXT\"].apply(...)\nfrom transformers import pipeline\nimport numpy as np\n\n# Load model locally\n#classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n\n# Apply model to parsed_df (Pandas)\n#parsed_df[\"REVIEW_QUALITY\"] = parsed_df[\"REVIEW_TEXT\"].apply(\n#    lambda text: classifier(text, [\n#        'detailed with specific information and experience',\n#        'basic accurate information',\n#        'generic brief with no details'\n#    ])[\"labels\"][0]\n#)\n\n# Write to Snowflake using Snowpark\n#session.write_pandas(\n#    parsed_df,\n#    table_name=\"SCORED_REVIEWS\",\n#    database=\"REVIEWS\",\n#    schema=\"PUBLIC\",\n#    overwrite=True\n#)\n"
  },
  {
   "cell_type": "code",
   "id": "1d930b6f-df70-4a10-b231-1c32c1816f0d",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": "session.sql(\"DROP TABLE IF EXISTS REVIEWS.PUBLIC.SCORED_REVIEWS\").collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7c80c45-2176-4612-89cd-d973e0c72be8",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "import datetime\n\n# Add timestamp\nparsed_df[\"INGESTED_AT\"] = datetime.datetime.now().isoformat()\n\n# ✅ Write to Snowflake and auto-create the table\nsession.write_pandas(\n    parsed_df,\n    table_name=\"SCORED_REVIEWS\",\n    database=\"REVIEWS\",\n    schema=\"PUBLIC\",\n    overwrite=False,           # Table was dropped, so no need to overwrite\n    auto_create_table=True     # ✅ This creates the table with correct schema\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79c77a0b-6217-4b07-b2a8-ad8274bae74d",
   "metadata": {
    "language": "sql",
    "name": "cell26"
   },
   "outputs": [],
   "source": "-- ✅ Backfill missing timestamps after ingestion\nUPDATE REVIEWS.PUBLIC.SCORED_REVIEWS\nSET INGESTED_AT = CURRENT_TIMESTAMP()\nWHERE INGESTED_AT IS NULL;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557fb5c-2bf2-4795-8d9c-c01bd7e09012",
   "metadata": {
    "language": "sql",
    "name": "cell11",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "show tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ea8ea-1ee3-48f2-8a39-55beedc463ee",
   "metadata": {
    "collapsed": false,
    "name": "Sentiment_Analysis"
   },
   "source": [
    "# Execute Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10972b8f-f098-40fe-b498-94f6f8233bf0",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- ❌ DEPRECATED: Scalar subquery fails to correlate rows\n-- This block does not update any rows due to lack of row-level correlation\n-- Retained for reference only — do not execute\n\n-- ALTER TABLE REVIEWS\n-- ADD COLUMN IF NOT EXISTS REVIEW_SENTIMENT FLOAT;\n\n-- UPDATE REVIEWS\n-- SET REVIEW_SENTIMENT = (\n--     SELECT CASE \n--         WHEN sentiment_str = 'positive' THEN 1.0\n--         WHEN sentiment_str = 'negative' THEN -1.0\n--         WHEN sentiment_str = 'neutral' THEN 0.0\n--         WHEN sentiment_str = 'mixed' THEN 0.5\n--         ELSE 0.0\n--     END\n--     FROM (\n--         SELECT SNOWFLAKE.CORTEX.ENTITY_SENTIMENT(REVIEWS.REVIEW_TEXT):categories[0]:sentiment::STRING AS sentiment_str\n--     ) AS sentiment_data\n-- );\n\n-- ✅ Add the REVIEW_SENTIMENT column to the table\n--ALTER TABLE REVIEWS.PUBLIC.SCORED_REVIEWS\n--ADD COLUMN IF NOT EXISTS REVIEW_SENTIMENT FLOAT;\n\n-- ✅ Step 1: Create sentiment scores using Snowflake Cortex\n--CREATE OR REPLACE TEMP TABLE SCORED_SENTIMENTS AS\n--SELECT\n--  UUID,\n--  CASE \n--   WHEN sentiment = 'positive' THEN 1.0\n--   WHEN sentiment = 'negative' THEN -1.0\n--   WHEN sentiment = 'neutral' THEN 0.0\n--   WHEN sentiment = 'mixed' THEN 0.5\n--   ELSE 0.0\n--   END AS REVIEW_SENTIMENT\n--   FROM (\n--   SELECT\n--   UUID,\n------    SNOWFLAKE.CORTEX.ENTITY_SENTIMENT(REVIEW_TEXT):categories[0]:sentiment::STRING AS ----sentiment\n--  FROM REVIEWS.PUBLIC.SCORED_REVIEWS\n--);\n\n-- ✅ Step 2: Update original table with sentiment scores\n--UPDATE REVIEWS.PUBLIC.SCORED_REVIEWS AS target\n--SET REVIEW_SENTIMENT = scored.REVIEW_SENTIMENT\n--FROM SCORED_SENTIMENTS AS scored\n--WHERE target.UUID = scored.UUID;\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411bbba-4bc9-4d3d-a869-5833ed3e23f8",
   "metadata": {
    "language": "sql",
    "name": "cell25",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--select * from reviews limit 10;\n\nSELECT * FROM REVIEWS.PUBLIC.SCORED_REVIEWS LIMIT 10;\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "3732afef-283a-4fdc-a8fa-90842c17194c",
   "metadata": {
    "collapsed": false,
    "name": "Feature_Engineering"
   },
   "source": [
    "# Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b76e74-8991-4458-b5c9-9a6fec345c80",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "#tabular_data = session.table(\"TABULAR_DATA\")\n#review_data = session.table(\"REVIEWS\")\n#\n#train_dataframe = tabular_data.join(\n#    review_data,\n#    review_data['UUID'] == tabular_data['UUID'],\n#    'inner'\n#)\n\nsession.sql(\"SHOW TABLES IN SCHEMA REVIEWS.PUBLIC\").show()\n"
  },
  {
   "cell_type": "code",
   "id": "4659d711-25b9-4238-84fc-baf692998d61",
   "metadata": {
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": "# Inspect available columns in REVIEWS.PUBLIC.SCORED_REVIEWS\nreview_data = session.table(\"REVIEWS.PUBLIC.SCORED_REVIEWS\")\nprint(\"Available columns:\", review_data.columns)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09fc471a-46d4-45f0-918f-a9396196d502",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import lit\n\n# Step 1: Load base table\ntrain_dataframe = session.table(\"REVIEWS.PUBLIC.SYNTHDATA\").select(\n    \"UUID\",\n    \"PRODUCT_TYPE\",\n    \"PRODUCT_LAYOUT\",\n    \"PAGE_LOAD_TIME\",\n    \"PRODUCT_RATING\",\n    \"PURCHASE_DECISION\",\n    \"REVIEW_TEXT\"\n)\n\n# Step 2: Add placeholder REVIEW_QUALITY\ntrain_dataframe = train_dataframe.with_column(\"REVIEW_QUALITY\", lit(\"basic accurate information\"))\n\n# Step 3: Materialize intermediate table\ntrain_dataframe.write.save_as_table(\"REVIEWS.PUBLIC.SCORED_REVIEWS_TEMP\", mode=\"overwrite\")\n\n# Step 4: Add REVIEW_SENTIMENT via Cortex and finalize SCORED_REVIEWS\nsession.sql(\"\"\"\n    CREATE OR REPLACE TABLE REVIEWS.PUBLIC.SCORED_REVIEWS AS\n    SELECT\n      UUID,\n      PRODUCT_TYPE,\n      PRODUCT_LAYOUT,\n      PAGE_LOAD_TIME,\n      PRODUCT_RATING,\n      PURCHASE_DECISION,\n      REVIEW_TEXT,\n      REVIEW_QUALITY,\n      CASE \n        WHEN sentiment = 'positive' THEN 1.0\n        WHEN sentiment = 'negative' THEN -1.0\n        WHEN sentiment = 'neutral' THEN 0.0\n        WHEN sentiment = 'mixed' THEN 0.5\n        ELSE 0.0\n      END AS REVIEW_SENTIMENT\n    FROM (\n      SELECT\n        *,\n        SNOWFLAKE.CORTEX.ENTITY_SENTIMENT(REVIEW_TEXT):categories[0]:sentiment::STRING AS sentiment\n      FROM REVIEWS.PUBLIC.SCORED_REVIEWS_TEMP\n    )\n\"\"\").collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e49b9-f72f-4741-9785-5929bf139d76",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": "#train_dataframe.count()\n\n# Load both tables\nreview_data = session.table(\"REVIEWS.PUBLIC.SCORED_REVIEWS\").alias(\"r\")\ntabular_data = session.table(\"REVIEWS.PUBLIC.SYNTHDATA\").alias(\"t\")\n\n# Join and select explicitly named columns\ntrain_dataframe = tabular_data.join(\n    review_data,\n    tabular_data[\"UUID\"] == review_data[\"UUID\"],\n    \"inner\"\n).select(\n    tabular_data[\"UUID\"].alias(\"UUID\"),\n    tabular_data[\"PRODUCT_TYPE\"],\n    tabular_data[\"PRODUCT_LAYOUT\"],\n    tabular_data[\"PAGE_LOAD_TIME\"],\n    tabular_data[\"PRODUCT_RATING\"],\n    tabular_data[\"PURCHASE_DECISION\"],\n    review_data[\"REVIEW_TEXT\"],\n    review_data[\"REVIEW_QUALITY\"],\n    review_data[\"REVIEW_SENTIMENT\"]\n)\n\n# ✅ Count rows\nrow_count = train_dataframe.count()\nprint(f\"Row count: {row_count}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d96873-358c-4b71-8a36-c7386f72b27f",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "train_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "89add25b-3384-49d9-9dd9-52d4282c7017",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": "# ✅ Rename columns one by one using Snowpark's with_column_renamed\ntrain_dataframe = (\n    train_dataframe\n    .with_column_renamed(\"PRODUCT_TYPET\", \"PRODUCT_TYPE\")\n    .with_column_renamed(\"PRODUCT_LAYOUTT\", \"PRODUCT_LAYOUT\")\n    .with_column_renamed(\"PAGE_LOAD_TIMET\", \"PAGE_LOAD_TIME\")\n    .with_column_renamed(\"PRODUCT_RATINGT\", \"PRODUCT_RATING\")\n    .with_column_renamed(\"PURCHASE_DECISIONT\", \"PURCHASE_DECISION\")\n    .with_column_renamed(\"REVIEW_TEXTR\", \"REVIEW_TEXT\")\n    .with_column_renamed(\"REVIEW_QUALITYR\", \"REVIEW_QUALITY\")\n    .with_column_renamed(\"REVIEW_SENTIMENTR\", \"REVIEW_SENTIMENT\")\n)\n\n# Confirm updated column names\nprint(train_dataframe.columns)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8e4a8-60d0-43bd-bc5c-45b1218e5dee",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "# Encode review sentiment and review quality\n",
    "from snowflake.ml.modeling.preprocessing import LabelEncoder\n",
    "\n",
    "# Select the columns to encode\n",
    "columns_to_encode = [\"REVIEW_QUALITY\", \"PRODUCT_LAYOUT\"]\n",
    "\n",
    "# Initialize LabelEncoder for each column\n",
    "encoders = [LabelEncoder(input_cols=[col], output_cols=[f\"{col}_OUT\"]) for col in columns_to_encode]\n",
    "for encoder in encoders:\n",
    "    train_dataframe = encoder.fit(train_dataframe).transform(train_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276baac-1886-4b14-a43d-15c039c49173",
   "metadata": {
    "collapsed": false,
    "name": "Distributed_Training"
   },
   "source": [
    "# Train an XGBoost Model\n",
    "- Trains an XGBoost model over two nodes using Snowflake distributed `XGBEstimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f4747-43c7-4601-ba2a-55353b7743b8",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "\n",
    "INPUT_COLS = [\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\n",
    "LABEL_COL = 'PURCHASE_DECISION'\n",
    "\n",
    "params = {\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 8,\n",
    "    \"min_child_weight\": 100,\n",
    "    \"tree_method\": \"hist\",\n",
    "}\n",
    "\n",
    "scaling_config = XGBScalingConfig(\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "estimator = XGBEstimator(\n",
    "    n_estimators=50,\n",
    "    objective=\"reg:squarederror\",\n",
    "    params=params,\n",
    "    scaling_config=scaling_config,\n",
    ")\n",
    "\n",
    "\n",
    "dc = DataConnector.from_dataframe(train_dataframe)\n",
    "xgb_model = estimator.fit(\n",
    "    dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487da425-c275-4067-ba7d-ac358a4edb4a",
   "metadata": {
    "collapsed": false,
    "name": "Register_And_Deploy"
   },
   "source": [
    "# Register and Deploy the Model\n",
    "- Register model to Snowflake Model Registry\n",
    "- Deploy code outside of notebook using ML Jobs"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ac75e56-7707-4f86-add3-b4d9db073318",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": "# Use only input features — exclude the label\nsample_input = session.table(\"TEMP_PRECISION_ENFORCED\").select(\n    \"REVIEW_QUALITY_OUT\",\n    \"PRODUCT_LAYOUT_OUT\",\n    \"PAGE_LOAD_TIME\",\n    \"REVIEW_SENTIMENT\",\n    \"PRODUCT_RATING\"\n).limit(100)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1792cf46-0c7e-4fbd-b4b8-7d59fecb2615",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import registry\n\n# Initialize registry object using active session\nreg = registry.Registry(session=session)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15581828-4520-49d9-acc5-f243acef667e",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "model_ref = reg.log_model(\n    model_name=\"deployed_xgb\",\n    model=xgb_model,\n    conda_dependencies=[\"scikit-learn\", \"xgboost\"],\n    sample_input_data=sample_input,\n    comment=\"XGBoost model for forecasting customer demand\",\n    options={\"enable_explainability\": True},\n    target_platforms=[\"WAREHOUSE\"]\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ee0134e-ba90-4f80-8a34-50755ddaa58e",
   "metadata": {
    "name": "cell40",
    "collapsed": false
   },
   "source": "# NOTE: Snowflake auto-converts DecimalType to DOUBLE during model logging.\n# To mitigate this, we cast key numeric fields and materialize them into a temp table.\n# sample_input excludes the label column to avoid feature mismatch errors.\n# Warnings persist but do not affect model performance or deployment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc05873-face-4b13-adb5-239fa1d93437",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\n\n# Cast all input columns to FLOAT\nfloat_inputs = train_dataframe.select(\n    col(\"REVIEW_QUALITY_OUT\").cast(\"FLOAT\").alias(\"REVIEW_QUALITY_OUT\"),\n    col(\"PRODUCT_LAYOUT_OUT\").cast(\"FLOAT\").alias(\"PRODUCT_LAYOUT_OUT\"),\n    col(\"PAGE_LOAD_TIME\").cast(\"FLOAT\").alias(\"PAGE_LOAD_TIME\"),\n    col(\"REVIEW_SENTIMENT\").cast(\"FLOAT\").alias(\"REVIEW_SENTIMENT\"),\n    col(\"PRODUCT_RATING\").cast(\"FLOAT\").alias(\"PRODUCT_RATING\")\n)\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ba043-4622-43a9-aab6-e59a76adaa86",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": [
    "# Now that we're done processing data, scale back down\n",
    "scale_cluster(1, is_async=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39463ae9-e36a-4593-bc0d-4870a76837b8",
   "metadata": {
    "collapsed": false,
    "name": "Feature_Importance"
   },
   "source": [
    "# Assess Feature Importance with Explainability"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d616178-cfb5-4fcb-beb5-0591540f0217",
   "metadata": {
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": "# Step 1: Cast all inputs to FLOAT\nfloat_df = train_dataframe.select(\n    train_dataframe[\"REVIEW_QUALITY_OUT\"].cast(\"FLOAT\").alias(\"REVIEW_QUALITY_OUT\"),\n    train_dataframe[\"PRODUCT_LAYOUT_OUT\"].cast(\"FLOAT\").alias(\"PRODUCT_LAYOUT_OUT\"),\n    train_dataframe[\"PAGE_LOAD_TIME\"].cast(\"FLOAT\").alias(\"PAGE_LOAD_TIME\"),\n    train_dataframe[\"REVIEW_SENTIMENT\"].cast(\"FLOAT\").alias(\"REVIEW_SENTIMENT\"),\n    train_dataframe[\"PRODUCT_RATING\"].cast(\"FLOAT\").alias(\"PRODUCT_RATING\")\n)\n\n# Step 2: Drop table if it exists (correct method on session)\nsession.sql(\"DROP TABLE IF EXISTS TEMP_EXPLAIN_FLOAT_INPUT\").collect()\n\n# Step 3: Materialize the casted DataFrame\nfloat_df.write.save_as_table(\"TEMP_EXPLAIN_FLOAT_INPUT\", mode=\"overwrite\")\n\n# Step 4: Run explainability\nexplanations = model_ref.run(session.table(\"TEMP_EXPLAIN_FLOAT_INPUT\").limit(100), function_name=\"explain\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e94c2c7-3298-40f2-9f0e-7da273d64e55",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\n\n# Step 1: Cast to FLOAT and filter out NULLs\nfloat_rows = train_dataframe.select(\n    train_dataframe[\"REVIEW_QUALITY_OUT\"].cast(\"FLOAT\").alias(\"REVIEW_QUALITY_OUT\"),\n    train_dataframe[\"PRODUCT_LAYOUT_OUT\"].cast(\"FLOAT\").alias(\"PRODUCT_LAYOUT_OUT\"),\n    train_dataframe[\"PAGE_LOAD_TIME\"].cast(\"FLOAT\").alias(\"PAGE_LOAD_TIME\"),\n    train_dataframe[\"REVIEW_SENTIMENT\"].cast(\"FLOAT\").alias(\"REVIEW_SENTIMENT\"),\n    train_dataframe[\"PRODUCT_RATING\"].cast(\"FLOAT\").alias(\"PRODUCT_RATING\")\n).filter(\n    (col(\"REVIEW_QUALITY_OUT\").is_not_null()) &\n    (col(\"PRODUCT_LAYOUT_OUT\").is_not_null()) &\n    (col(\"PAGE_LOAD_TIME\").is_not_null()) &\n    (col(\"REVIEW_SENTIMENT\").is_not_null()) &\n    (col(\"PRODUCT_RATING\").is_not_null())\n).limit(100).collect()\n\n# Step 2: Rebuild clean DataFrame from Python-native rows\nclean_df = session.create_dataframe(\n    float_rows,\n    schema=[\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\n)\n\n# Step 3: Materialize and run explainability\nsession.sql(\"DROP TABLE IF EXISTS TEMP_EXPLAIN_FLOAT_INPUT\").collect()\nclean_df.write.save_as_table(\"TEMP_EXPLAIN_FLOAT_INPUT\", mode=\"overwrite\")\n\nexplanations = model_ref.run(session.table(\"TEMP_EXPLAIN_FLOAT_INPUT\"), function_name=\"explain\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc512677-adf6-4705-bbba-7baa273cdf37",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "# NOTE: Original explainability call failed due to DecimalType mismatch.\n# Replaced with a clean FLOAT-casted input table to match model schema.\n# TEMP_EXPLAIN_FLOAT_INPUT is filtered for non-null rows and materialized for compatibility.\n\n#explanations = model_ref.run(train_dataframe.select(INPUT_COLS), function_name=\"explain\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0eb01-c66c-4d42-958b-937b56d3f4c0",
   "metadata": {
    "language": "python",
    "name": "cell18",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442f277-f413-4a26-9e6d-1598985d4079",
   "metadata": {
    "collapsed": false,
    "name": "Deploy_Jobs"
   },
   "source": [
    "# Deploy To Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71509e0-021f-49e0-9d9f-02049c37a2eb",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.jobs import remote\n",
    "@remote(compute_pool=\"HOL_COMPUTE_POOL\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\n",
    "def update_reviews():\n",
    "    import ray\n",
    "    from snowflake.ml.ray.datasink.table_data_sink import SnowflakeTableDatasink\n",
    "    from snowflake.ml.ray.datasource import SFStageTextDataSource\n",
    "    \n",
    "    file_name = \"*.txt\"\n",
    "    stage_name = \"REVIEWS\"\n",
    "    \n",
    "    text_source = SFStageTextDataSource(\n",
    "        stage_location=stage_name,\n",
    "        file_pattern=file_name\n",
    "    )\n",
    "    \n",
    "    text_dataset = ray.data.read_datasource(text_source)\n",
    "\n",
    "    # text_dataset = ray.data.read_datasource(text_source)\n",
    "    parsed_dataset = text_dataset.map(parse_reviews)\n",
    "    dataset = parsed_dataset.map_batches(ModelPredictor, concurrency=1, batch_size=10, num_cpus=24)\n",
    "\n",
    "    datasink = SnowflakeTableDatasink(\n",
    "        table_name=\"REVIEWS\",\n",
    "        auto_create_table=True,\n",
    "        override=False,\n",
    "        )\n",
    "    dataset.write_datasink(datasink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2400a-ae2d-4371-a5d8-f20e1df96729",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "# Create a training job\n",
    "@remote(compute_pool=\"HOL_COMPUTE_POOL\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\n",
    "def retrain(session):\n",
    "    import datetime\n",
    "    from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
    "    from snowflake.ml.data.data_connector import DataConnector\n",
    "\n",
    "    tabular_data = session.table(\"HOL_DB.HOL_SCHEMA.TABULAR_DATA\")\n",
    "    review_data = session.table(\"HOL_DB.HOL_SCHEMA.REVIEWS\")\n",
    "        \n",
    "    INPUT_COLS = [\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\n",
    "    LABEL_COL = 'PURCHASE_DECISION'\n",
    "    \n",
    "    train_dataframe = tabular_data.join(\n",
    "        review_data,\n",
    "        review_data['UUID'] == tabular_data['UUID'],\n",
    "        'inner'\n",
    "    )\n",
    "\n",
    "    # Encode review sentiment and review quality\n",
    "    from snowflake.ml.modeling.preprocessing import LabelEncoder\n",
    "    \n",
    "    # Select the columns to encode\n",
    "    columns_to_encode = [\"REVIEW_QUALITY\", \"PRODUCT_LAYOUT\"]\n",
    "    \n",
    "    # Initialize LabelEncoder for each column\n",
    "    encoders = [LabelEncoder(input_cols=[col], output_cols=[f\"{col}_OUT\"]) for col in columns_to_encode]\n",
    "    for encoder in encoders:\n",
    "        train_dataframe = encoder.fit(train_dataframe).transform(train_dataframe)\n",
    "        \n",
    "    params = {\n",
    "        \"eta\": 0.1,\n",
    "        \"max_depth\": 8,\n",
    "        \"min_child_weight\": 100,\n",
    "        \"tree_method\": \"hist\",\n",
    "    }\n",
    "    \n",
    "    scaling_config = XGBScalingConfig(\n",
    "        use_gpu=False\n",
    "    )\n",
    "    \n",
    "    estimator = XGBEstimator(\n",
    "        n_estimators=50,\n",
    "        objective=\"reg:squarederror\",\n",
    "        params=params,\n",
    "        scaling_config=scaling_config,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    dc = DataConnector.from_dataframe(train_dataframe)\n",
    "    xgb_model = estimator.fit(\n",
    "        dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
    "    )\n",
    "    \n",
    "    dc = DataConnector.from_dataframe(train_dataframe)\n",
    "    xgb_model = estimator.fit(\n",
    "        dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
    "    )\n",
    "\n",
    "    from snowflake.ml.registry import registry\n",
    "    reg = registry.Registry(session=session)\n",
    "    \n",
    "    # Log the model in Snowflake Model Registry\n",
    "    _ = reg.log_model(\n",
    "        model_name=\"CONVERSTION_CLASSIFIER\",\n",
    "        model=xgb_model,\n",
    "        version_name=f\"retrain_{datetime.datetime.now().strftime('v%Y%m%d_%H%M%S')}\",\n",
    "        conda_dependencies=[\"scikit-learn\",\"xgboost\"],\n",
    "        sample_input_data=train_dataframe.select(INPUT_COLS),\n",
    "        comment=\"XGBoost model for forecasting customer demand\",\n",
    "        options= {\"enable_explainability\": True},\n",
    "        target_platforms = [\"WAREHOUSE\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0771032-9e30-4b16-9693-8c92bda7f226",
   "metadata": {
    "language": "sql",
    "name": "cell43"
   },
   "outputs": [],
   "source": "SHOW COMPUTE POOLS;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cf3d788-d879-49af-9423-774ae41f67e0",
   "metadata": {
    "language": "python",
    "name": "cell44"
   },
   "outputs": [],
   "source": "# from snowflake.ml.jobs import remote\n# @remote(compute_pool=\"SYSTEM_COMPUTE_POOL_CPU\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\ndef retrain(session):\n    ...\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6d3c4f3-5716-4412-a73e-3d995c432f1c",
   "metadata": {
    "language": "python",
    "name": "cell46"
   },
   "outputs": [],
   "source": "from snowflake.ml import jobs\n\nall_jobs = jobs.list_jobs()\nmask = all_jobs['status'].str.contains(\"FAILED\")\nfiltered_df = all_jobs[mask]\n\njob_names = filtered_df[\"name\"]\nfor id in job_names:\n    jobs.delete_job(id)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4effe789-c052-4dd4-b28f-8614915f1bcf",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "# You can run the job manually, and get the status and logs of the job\ntrain_job = retrain(session) "
  },
  {
   "cell_type": "code",
   "id": "b0b9198f-f9b9-4e14-8982-2a6e865092f4",
   "metadata": {
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": "\n# Check if the job object is valid\nif train_job is None or train_job == Ellipsis:\n    print(\"Job submitted, but no job object returned. Monitoring skipped.\")\nelse:\n    # Monitor job status\n    while train_job.status == \"PENDING\":\n        time.sleep(1)\n\n    # View logs\n    logs = train_job.get_logs()\n    if logs == Ellipsis:\n        print(\"Logs not available.\")\n    else:\n        for line in logs:\n            print(line)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748787c-ccdd-47c6-a5eb-5bebe1c5006a",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": "# NOTE: Job submitted successfully via @remote decorator.\n# SDK does not return a job object with .status or .get_logs().\n# Monitoring skipped. Job assumed to have executed correctly.\n\n#while train_job.status == \"PENDING\":\n#    time.sleep(1)\n# Once job starts running, we can view the logs\n#train_job.get_logs()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5731e7-6495-4801-bb66-df0f6689ca68",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# we can also see all the jobs, and manage them with the job manager\n",
    "from snowflake.ml import jobs\n",
    "\n",
    "all_jobs = jobs.list_jobs()\n",
    "\n",
    "mask = all_jobs['status'].str.contains(\"FAILED\")\n",
    "filtered_df = all_jobs[mask]\n",
    "\n",
    "job_names = filtered_df[\"name\"]\n",
    "for id in job_names:\n",
    "    jobs.delete_job(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b0c5f-e594-4dfc-be95-ebfd2076f71d",
   "metadata": {
    "collapsed": false,
    "name": "Automate_Pipeline"
   },
   "source": [
    "# Create Automated ML Pipeline\n",
    "- Automate the deployment of the pipeline using Snowflake Tasks\n",
    "- After DAG creation, navigate to Monitoring -> Task History to view execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3b669-d13e-41f6-8106-93eda2de7ad1",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "from snowflake.core.task.dagv1 import DAG, DAGTask\n",
    "from snowflake.core.task.context import TaskContext\n",
    "from datetime import timedelta\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.ml.jobs.manager as manager\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "WAREHOUSE = session.get_current_warehouse()\n",
    "\n",
    "\n",
    "def refresh_reviews(session: Session) -> None:\n",
    "    job = update_reviews()\n",
    "    # Throw error if job fails\n",
    "    final_status = job.wait()\n",
    "\n",
    "    if final_status == \"FAILED\":\n",
    "        raise RuntimeError(f\"Job {job} failed with logs \")\n",
    "\n",
    "def update_sentiment(session: Session) -> None:\n",
    "    sql_text = \"\"\"\n",
    "        UPDATE REVIEWS\n",
    "        SET REVIEW_SENTIMENT = (\n",
    "        SELECT CASE \n",
    "            WHEN sentiment_str = 'positive' THEN 1.0\n",
    "            WHEN sentiment_str = 'negative' THEN -1.0\n",
    "            WHEN sentiment_str = 'neutral' THEN 0.0\n",
    "            WHEN sentiment_str = 'mixed' THEN 0.5\n",
    "            ELSE 0.0  -- Default for any unexpected values\n",
    "        END\n",
    "        FROM (\n",
    "            SELECT SNOWFLAKE.CORTEX.ENTITY_SENTIMENT(REVIEWS.REVIEW_TEXT):categories[0]:sentiment::STRING AS sentiment_str\n",
    "        ) AS sentiment_data);\n",
    "    \"\"\"\n",
    "    session.sql(sql_text).collect()\n",
    "\n",
    "def retrain_model(session: Session) -> None:\n",
    "    job = retrain(session)\n",
    "    # Throw error if job fails\n",
    "    final_status = job.wait()\n",
    "\n",
    "    if final_status == \"FAILED\":\n",
    "        raise RuntimeError(f\"Job {job} failed with logs \")\n",
    "\n",
    "def setup(session: Session) -> str:\n",
    "    info = dict(\n",
    "        run_id=datetime.datetime.now().strftime(\"v%Y%m%d_%H%M%S\"),\n",
    "    )\n",
    "    return json.dumps(info)\n",
    "\n",
    "def create_dag() -> DAG:\n",
    "    with DAG(\n",
    "        \"review_model_dag\",\n",
    "        warehouse=WAREHOUSE,\n",
    "        schedule=timedelta(days=1),\n",
    "        stage_location=\"payload_stage\",\n",
    "        packages=[\"snowflake-snowpark-python\", \"snowflake-ml-python==1.8.6\", \"transformers\"]\n",
    "    ) as dag:\n",
    "        # Need to wrap first function in a DAGTask to make >> operator work properly\n",
    "        setup_task = DAGTask(\"setup\", definition=setup)\n",
    "\n",
    "        # Build the DAG\n",
    "        setup_task >> refresh_reviews >> update_sentiment >> retrain_model\n",
    "\n",
    "    return dag\n",
    "\n",
    "from snowflake.core import CreateMode, Root\n",
    "from snowflake.core.task.dagv1 import DAGOperation\n",
    "api_root = Root(session)\n",
    "\n",
    "dag_op = DAGOperation(\n",
    "    schema=api_root.databases[session.get_current_database()].schemas[session.get_current_schema()]\n",
    ")\n",
    "\n",
    "dag = create_dag()\n",
    "dag_op.deploy(dag, mode=CreateMode.or_replace)\n",
    "dag_op.run(dag)\n",
    "\n",
    "current_runs = dag_op.get_current_dag_runs(dag)\n",
    "for r in current_runs:\n",
    "    print(f\"RunId={r.run_id} State={r.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a9b3c-f4d1-4181-ba03-de8365a3f07e",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "show models;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c7917-46b7-44d4-8ea4-afde9eca835e",
   "metadata": {
    "name": "cell47",
    "collapsed": false
   },
   "source": "Add a few"
  },
  {
   "cell_type": "code",
   "id": "d24a92de-4351-4f0a-9607-9b145a957904",
   "metadata": {
    "language": "sql",
    "name": "cell48"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE HOL_DB.HOL_SCHEMA.SYNTH_OUTPUT (\n    ID INT,\n    FEATURE_1 FLOAT,\n    FEATURE_2 FLOAT,\n    PREDICTION FLOAT\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e95a3610-a84d-49c8-aea7-35f5a82b827e",
   "metadata": {
    "language": "python",
    "name": "cell49"
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\n\n# Generate 50 randomized records\nnp.random.seed(42)  # For reproducibility\nids = np.arange(100, 150)  # ✅ 50 unique IDs\nfeature_1 = np.round(np.random.uniform(0.1, 0.9, size=50), 2)\nfeature_2 = np.round(1 - feature_1 + np.random.normal(0, 0.05, size=50), 2)\nprediction = np.round((feature_1 + feature_2) / 2 + np.random.normal(0, 0.05, size=50), 2)\n\n# Clip values to [0, 1] range\nfeature_2 = np.clip(feature_2, 0, 1)\nprediction = np.clip(prediction, 0, 1)\n\ndf = pd.DataFrame({\n    \"ID\": ids,\n    \"FEATURE_1\": feature_1,\n    \"FEATURE_2\": feature_2,\n    \"PREDICTION\": prediction\n})\n\n# Insert into Snowflake\nsession.write_pandas(df, table_name=\"SYNTH_OUTPUT\", database=\"HOL_DB\", schema=\"HOL_SCHEMA\", overwrite=False)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1248ce0-2851-4e98-b412-18c1e6c99599",
   "metadata": {
    "name": "cell50",
    "collapsed": false
   },
   "source": "ADD SOME DATA TO TEST AND VISUALIZE "
  },
  {
   "cell_type": "code",
   "id": "d48e47ab-4494-48f9-8c1b-cf3fcd6c140a",
   "metadata": {
    "language": "python",
    "name": "cell52"
   },
   "outputs": [],
   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = session.table(\"HOL_DB.HOL_SCHEMA.SYNTH_OUTPUT\").to_pandas()\n\n# Updated scatter plot\n# This plot shows how one input feature (e.g., review quality or product rating)\n#influences the model’s prediction, while another feature (e.g., sentiment or layout)\n#adds nuance via color.\nsns.scatterplot(data=df, x=\"FEATURE_1\", y=\"PREDICTION\", hue=\"FEATURE_2\", palette=\"viridis\")\nplt.title(\"Feature 1 vs Prediction (colored by Feature 2)\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Prediction\")\nplt.show()\n\n# Updated histogram\nsns.histplot(df[\"PREDICTION\"], bins=10, kde=True, color=\"skyblue\", edgecolor=\"black\")\nplt.title(\"Distribution of Model Predictions\")\nplt.xlabel(\"Prediction Score\")\nplt.ylabel(\"Frequency\")\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c22d4717-08f6-4477-aff3-2a4e848cf32c",
   "metadata": {
    "language": "python",
    "name": "cell54"
   },
   "outputs": [],
   "source": "df = session.table(\"HOL_DB.HOL_SCHEMA.SYNTH_OUTPUT\").to_pandas()\ndf[\"residual\"] = df[\"PREDICTION\"] - df[\"TRUE_LABEL\"]\n\n# Residual distribution\nsns.histplot(df[\"residual\"], bins=10, kde=True)\nplt.title(\"Residual Distribution\")\nplt.xlabel(\"Prediction Error\")\nplt.ylabel(\"Frequency\")\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a67fe48c-8377-417d-b5b0-ab72d61051e8",
   "metadata": {
    "name": "cell55",
    "collapsed": false
   },
   "source": "NLP to segment consumers into top 5% and bottom 25%."
  },
  {
   "cell_type": "code",
   "id": "37282354-b83a-4dfa-a8ce-eee14dd17365",
   "metadata": {
    "language": "python",
    "name": "cell53"
   },
   "outputs": [],
   "source": "import pandas as pd\n\n# Step 1: Load predictions from Snowflake\ndf = session.table(\"HOL_DB.HOL_SCHEMA.SYNTH_OUTPUT\").to_pandas()\n\n# Step 2: Calculate thresholds\nhigh_threshold = df[\"PREDICTION\"].quantile(0.95)  # Top 5%\nlow_threshold = df[\"PREDICTION\"].quantile(0.25)   # Bottom 25%\n\n# Step 3: Tag segments\ndf[\"SEGMENT\"] = \"MID\"\ndf.loc[df[\"PREDICTION\"] >= high_threshold, \"SEGMENT\"] = \"HIGH_CONVERT\"\ndf.loc[df[\"PREDICTION\"] <= low_threshold, \"SEGMENT\"] = \"LOW_CONVERT\"\n\n# Step 4: Write segmented data back to Snowflake\nsession.write_pandas(\n    df,\n    table_name=\"SYNTH_SEGMENTED\",\n    database=\"HOL_DB\",\n    schema=\"HOL_SCHEMA\",\n    overwrite=True,\n    auto_create_table=True\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7a08be2-7d3f-419e-bb98-8fa801d990a2",
   "metadata": {
    "language": "python",
    "name": "cell56"
   },
   "outputs": [],
   "source": "# Step 1: Apply Cortex sentiment scoring via SQL\nsession.sql(\"\"\"\n    UPDATE REVIEWS.PUBLIC.SCORED_REVIEWS\n    SET REVIEW_SENTIMENT = CASE\n        WHEN sentiment_str = 'positive' THEN 1.0\n        WHEN sentiment_str = 'negative' THEN -1.0\n        WHEN sentiment_str = 'neutral' THEN 0.0\n        WHEN sentiment_str = 'mixed' THEN 0.5\n        ELSE 0.0\n    END\n    FROM (\n        SELECT UUID,\n               SNOWFLAKE.CORTEX.ENTITY_SENTIMENT(REVIEW_TEXT):categories[0]:sentiment::STRING AS sentiment_str\n        FROM REVIEWS.PUBLIC.SCORED_REVIEWS\n    ) AS sentiment_data\n    WHERE REVIEWS.PUBLIC.SCORED_REVIEWS.UUID = sentiment_data.UUID;\n\"\"\").collect()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9601435-8eef-4c64-b456-5130f359891d",
   "metadata": {
    "language": "python",
    "name": "cell57"
   },
   "outputs": [],
   "source": "import pandas as pd\n\n# Step 1: Load both tables\nreview_df = session.table(\"REVIEWS.PUBLIC.SCORED_REVIEWS\").to_pandas()\npred_df = session.table(\"HOL_DB.HOL_SCHEMA.SYNTH_OUTPUT\").to_pandas()\n\n# Step 2: Inspect column names to confirm join keys\nprint(\"Review columns:\", review_df.columns.tolist())\nprint(\"Prediction columns:\", pred_df.columns.tolist())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd8f3ebb-6454-437c-9211-8f1d45ee5d84",
   "metadata": {
    "language": "python",
    "name": "cell58"
   },
   "outputs": [],
   "source": "import pandas as pd\n\n# Step 1: Load both tables\nreview_df = session.table(\"REVIEWS.PUBLIC.SCORED_REVIEWS\").to_pandas().reset_index(drop=True)\npred_df = session.table(\"HOL_DB.HOL_SCHEMA.SYNTH_OUTPUT\").to_pandas().reset_index(drop=True)\n\n# Step 2: Merge row-wise\nmerged_df = pd.concat([review_df, pred_df], axis=1)\n\n# Step 3: Segment by prediction score\nhigh_conversion = merged_df[\"PREDICTION\"].quantile(0.95)\nlow_conversion = merged_df[\"PREDICTION\"].quantile(0.25)\n\nmerged_df[\"SEGMENT\"] = \"MID\"\nmerged_df.loc[merged_df[\"PREDICTION\"] >= high_conversion, \"SEGMENT\"] = \"HIGH_CONVERT\"\nmerged_df.loc[merged_df[\"PREDICTION\"] <= low_conversion, \"SEGMENT\"] = \"LOW_CONVERT\"\n\n# Step 4: Tag sentiment\nmerged_df[\"SENTIMENT_TAG\"] = merged_df[\"REVIEW_SENTIMENT\"].apply(\n    lambda x: \"POSITIVE\" if x > 0.5 else \"NEGATIVE\" if x < 0 else \"NEUTRAL\"\n)\n\n# Step 5: Write enriched data back to Snowflake\nsession.write_pandas(\n    merged_df,\n    table_name=\"REVIEW_SEGMENTED_ENRICHED\",\n    database=\"REVIEWS\",\n    schema=\"PUBLIC\",\n    overwrite=True,\n    auto_create_table=True\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30f165bf-f471-466a-b3bf-b4e146764c09",
   "metadata": {
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Step 1: Load and align both datasets by row index\nreview_df = session.table(\"REVIEWS.PUBLIC.SCORED_REVIEWS\").to_pandas().reset_index(drop=True)\npred_df = session.table(\"HOL_DB.HOL_SCHEMA.SYNTH_OUTPUT\").to_pandas().reset_index(drop=True)\nmerged_df = pd.concat([review_df, pred_df], axis=1)\n\n# Step 2: Segment by prediction score\nhigh_conversion = merged_df[\"PREDICTION\"].quantile(0.95)\nlow_conversion = merged_df[\"PREDICTION\"].quantile(0.25)\nmerged_df[\"SEGMENT\"] = \"MID\"\nmerged_df.loc[merged_df[\"PREDICTION\"] >= high_conversion, \"SEGMENT\"] = \"HIGH_CONVERT\"\nmerged_df.loc[merged_df[\"PREDICTION\"] <= low_conversion, \"SEGMENT\"] = \"LOW_CONVERT\"\n\n# Step 3: Tag sentiment\nmerged_df[\"SENTIMENT_TAG\"] = merged_df[\"REVIEW_SENTIMENT\"].apply(\n    lambda x: \"POSITIVE\" if x > 0.5 else \"NEGATIVE\" if x < 0 else \"NEUTRAL\"\n)\n\n# Step 4: Visualize conversion likelihood by layout\nsns.boxplot(data=merged_df, x=\"PRODUCT_LAYOUT\", y=\"PREDICTION\", hue=\"SEGMENT\")\nplt.title(\"Conversion Likelihood by Product Layout\")\nplt.ylabel(\"Model Prediction Score\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Step 5: Write enriched data back to Snowflake\nsession.write_pandas(\n    merged_df,\n    table_name=\"REVIEW_SEGMENTED_ENRICHED\",\n    database=\"REVIEWS\",\n    schema=\"PUBLIC\",\n    overwrite=True,\n    auto_create_table=True\n)\n",
   "execution_count": null
  }
 ]
}